{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI 감정 인식 모델 학습 (병렬 처리 & 정규화)\n",
                "\n",
                "이 노트북은 **MediaPipe FaceMesh**를 사용하여 얼굴 랜드마크를 추출하고, PyTorch를 사용하여 감정 분류 모델을 학습합니다.\n",
                "\n",
                "**개선된 점:**\n",
                "- 2개의 GPU 활용 (DataParallel)\n",
                "- **병렬 데이터 처리 (Parallel Processing)**: ProcessPoolExecutor를 사용하여 데이터 로딩 속도 대폭 향상\n",
                "- **좌표 정규화 (Coordinate Normalization)**: 얼굴 위치와 크기에 상관없이 학습되도록 전처리 추가\n",
                "- 학습 진행 상황(Loss, Accuracy) 실시간 시각화"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "Detected 2 GPUs! Using DataParallel.\n"
                    ]
                }
            ],
            "source": [
                "import zipfile\n",
                "import cv2\n",
                "import mediapipe as mp\n",
                "import numpy as np\n",
                "import os\n",
                "import joblib\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "from concurrent.futures import ProcessPoolExecutor\n",
                "\n",
                "# 시각화 설정\n",
                "%matplotlib inline\n",
                "\n",
                "# GPU 설정 확인\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.device_count() > 1:\n",
                "    print(f\"Detected {torch.cuda.device_count()} GPUs! Using DataParallel.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================\n",
                "# 설정 변수\n",
                "# ============================\n",
                "\n",
                "# 데이터셋 경로 (사용자 환경에 맞게 수정됨)\n",
                "BASE_DIR = r\"c:\\Users\\ldy34\\Desktop\\Face\\video\"\n",
                "TRAIN_DIR = os.path.join(BASE_DIR, \"Training\")\n",
                "\n",
                "# 학습할 파일 목록\n",
                "TARGET_FILES = {\n",
                "    \"Neutral\": {\"filename\": \"[원천]EMOIMG_중립_TRAIN_01.zip\", \"label\": 0},\n",
                "    \"Anxious\": {\"filename\": \"[원천]EMOIMG_불안_TRAIN_01.zip\", \"label\": 1}\n",
                "}\n",
                "\n",
                "# 추출할 최대 샘플 수 (테스트용 10,000개)\n",
                "MAX_SAMPLES = 10000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================\n",
                "# 병렬 처리를 위한 함수 정의\n",
                "# ============================\n",
                "\n",
                "def process_single_image(args):\n",
                "    \"\"\"\n",
                "    이미지 데이터(bytes) 하나를 받아서 랜드마크를 추출하고 정규화하는 함수\n",
                "    (ProcessPoolExecutor에서 실행됨)\n",
                "    \"\"\"\n",
                "    img_data, label = args\n",
                "    try:\n",
                "        img_array = np.frombuffer(img_data, np.uint8)\n",
                "        image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
                "        if image is None: return None\n",
                "        \n",
                "        # FaceMesh 객체를 프로세스마다 새로 생성해야 충돌이 없음\n",
                "        with mp.solutions.face_mesh.FaceMesh(\n",
                "            static_image_mode=True, \n",
                "            max_num_faces=1, \n",
                "            refine_landmarks=True, \n",
                "            min_detection_confidence=0.5\n",
                "        ) as face_mesh_local:\n",
                "            \n",
                "            results = face_mesh_local.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
                "            \n",
                "            if results.multi_face_landmarks:\n",
                "                landmarks_raw = []\n",
                "                for lm in results.multi_face_landmarks[0].landmark:\n",
                "                    landmarks_raw.append([lm.x, lm.y, lm.z])\n",
                "                \n",
                "                landmarks_raw = np.array(landmarks_raw)\n",
                "                \n",
                "                # 1. 중심 이동\n",
                "                nose_tip = landmarks_raw[1]\n",
                "                landmarks_centered = landmarks_raw - nose_tip\n",
                "                \n",
                "                # 2. 스케일링\n",
                "                max_dist = np.max(np.linalg.norm(landmarks_centered, axis=1))\n",
                "                if max_dist > 0:\n",
                "                    landmarks_normalized = landmarks_centered / max_dist\n",
                "                else:\n",
                "                    landmarks_normalized = landmarks_centered\n",
                "                \n",
                "                return landmarks_normalized.flatten(), label\n",
                "\n",
                "    except Exception:\n",
                "        return None\n",
                "    return None\n",
                "\n",
                "def extract_landmarks_parallel(zip_path, label, max_samples=10000):\n",
                "    if not os.path.exists(zip_path):\n",
                "        print(f\"\\n[Error] File not found: {zip_path}\")\n",
                "        return [], []\n",
                "\n",
                "    print(f\"\\n[{os.path.basename(zip_path)}] Reading Zip...\")\n",
                "    \n",
                "    # 1. ZIP 파일 읽기 (메인 스레드)\n",
                "    image_data_list = []\n",
                "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
                "        file_list = [f for f in z.namelist() if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
                "        use_count = min(len(file_list), max_samples)\n",
                "        target_files = file_list[:use_count]\n",
                "        \n",
                "        # 이미지를 메모리에 로드 (병렬 처리를 위해)\n",
                "        for img_name in target_files:\n",
                "            with z.open(img_name) as f:\n",
                "                image_data_list.append((f.read(), label))\n",
                "    \n",
                "    print(f\"  - Loaded {len(image_data_list):,} images. Starting CPU Parallel Processing...\")\n",
                "\n",
                "    # 2. 병렬 처리 실행\n",
                "    data = []\n",
                "    labels = []\n",
                "    \n",
                "    # max_workers는 CPU 코어 수만큼 자동 할당됨\n",
                "    with ProcessPoolExecutor() as executor:\n",
                "        # map 함수로 병렬 실행\n",
                "        results = list(tqdm(executor.map(process_single_image, image_data_list), total=len(image_data_list), desc=\"  Processing\"))\n",
                "        \n",
                "        for res in results:\n",
                "            if res is not None:\n",
                "                d, l = res\n",
                "                data.append(d)\n",
                "                labels.append(l)\n",
                "                \n",
                "    print(f\"  -> Successfully extracted: {len(data):,} samples\")\n",
                "    return data, labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Parallel Data Extraction...\n",
                        "\n",
                        "[[원천]EMOIMG_중립_TRAIN_01.zip] Reading Zip...\n",
                        "  - Loaded 10,000 images. Starting CPU Parallel Processing...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Processing:   0%|          | 0/10000 [00:00<?, ?it/s]\n"
                    ]
                },
                {
                    "ename": "BrokenProcessPool",
                    "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m emotion, info \u001b[38;5;129;01min\u001b[39;00m TARGET_FILES.items():\n\u001b[32m      9\u001b[39m     path = os.path.join(TRAIN_DIR, info[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     d, l = \u001b[43mextract_landmarks_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     X.extend(d)\n\u001b[32m     12\u001b[39m     y.extend(l)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mextract_landmarks_parallel\u001b[39m\u001b[34m(zip_path, label, max_samples)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# max_workers는 CPU 코어 수만큼 자동 할당됨\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# map 함수로 병렬 실행\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_single_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_data_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m  Processing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ldy34\\Desktop\\Face\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\StabilityMatrix-win-x64\\Data\\Assets\\Python\\cpython-3.12.11-windows-x86_64-none\\Lib\\concurrent\\futures\\process.py:636\u001b[39m, in \u001b[36m_chain_from_iterable_of_lists\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[33;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[32m    634\u001b[39m \u001b[33;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\StabilityMatrix-win-x64\\Data\\Assets\\Python\\cpython-3.12.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\StabilityMatrix-win-x64\\Data\\Assets\\Python\\cpython-3.12.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\StabilityMatrix-win-x64\\Data\\Assets\\Python\\cpython-3.12.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\StabilityMatrix-win-x64\\Data\\Assets\\Python\\cpython-3.12.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[31mBrokenProcessPool\u001b[39m: A process in the process pool was terminated abruptly while the future was running or pending."
                    ]
                }
            ],
            "source": [
                "# 데이터 로드 실행\n",
                "if __name__ == '__main__':\n",
                "    X = []\n",
                "    y = []\n",
                "\n",
                "    print(\"Starting Parallel Data Extraction...\")\n",
                "\n",
                "    for emotion, info in TARGET_FILES.items():\n",
                "        path = os.path.join(TRAIN_DIR, info['filename'])\n",
                "        d, l = extract_landmarks_parallel(path, info['label'], max_samples=MAX_SAMPLES)\n",
                "        X.extend(d)\n",
                "        y.extend(l)\n",
                "\n",
                "    X = np.array(X)\n",
                "    y = np.array(y)\n",
                "\n",
                "    print(f\"\\nTotal Dataset Size: {len(X):,} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 데이터 분할 및 텐서 변환\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "# Tensor 변환\n",
                "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
                "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
                "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
                "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
                "\n",
                "# DataLoader (배치 사이즈 증가)\n",
                "BATCH_SIZE = 128\n",
                "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Train Set: {len(X_train):,} | Test Set: {len(X_test):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 모델 정의\n",
                "class EmotionMLP(nn.Module):\n",
                "    def __init__(self, input_size):\n",
                "        super(EmotionMLP, self).__init__()\n",
                "        self.network = nn.Sequential(\n",
                "            nn.Linear(input_size, 512),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(512),\n",
                "            nn.Dropout(0.3),\n",
                "            \n",
                "            nn.Linear(512, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.Dropout(0.2),\n",
                "            \n",
                "            nn.Linear(256, 128),\n",
                "            nn.ReLU(),\n",
                "            \n",
                "            nn.Linear(128, 2) # Neutral, Anxious\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.network(x)\n",
                "\n",
                "model = EmotionMLP(X_train.shape[1]).to(device)\n",
                "\n",
                "# Multi-GPU 적용\n",
                "if torch.cuda.device_count() > 1:\n",
                "    model = nn.DataParallel(model)\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "print(\"Model Initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 학습 루프\n",
                "epochs = 50\n",
                "history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
                "\n",
                "print(\"Target Epochs:\", epochs)\n",
                "for epoch in range(epochs):\n",
                "    # --- Training ---\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for X_batch, y_batch in train_loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(X_batch)\n",
                "        loss = criterion(outputs, y_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item()\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        total += y_batch.size(0)\n",
                "        correct += (predicted == y_batch).sum().item()\n",
                "    \n",
                "    epoch_train_loss = running_loss / len(train_loader)\n",
                "    epoch_train_acc = correct / total\n",
                "    \n",
                "    # --- Validation (Test) ---\n",
                "    model.eval()\n",
                "    test_loss = 0.0\n",
                "    correct_test = 0\n",
                "    total_test = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for X_batch, y_batch in test_loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            outputs = model(X_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            \n",
                "            test_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total_test += y_batch.size(0)\n",
                "            correct_test += (predicted == y_batch).sum().item()\n",
                "            \n",
                "    epoch_test_loss = test_loss / len(test_loader)\n",
                "    epoch_test_acc = correct_test / total_test\n",
                "    \n",
                "    history['train_loss'].append(epoch_train_loss)\n",
                "    history['test_loss'].append(epoch_test_loss)\n",
                "    history['train_acc'].append(epoch_train_acc)\n",
                "    history['test_acc'].append(epoch_test_acc)\n",
                "    \n",
                "    if (epoch+1) % 5 == 0:\n",
                "        print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
                "              f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
                "              f\"Test Loss: {epoch_test_loss:.4f} Acc: {epoch_test_acc:.4f}\")\n",
                "\n",
                "print(\"Training Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 결과 시각화\n",
                "plt.figure(figsize=(14, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history['train_loss'], label='Train Loss')\n",
                "plt.plot(history['test_loss'], label='Test (Validation) Loss')\n",
                "plt.title('Loss')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history['train_acc'], label='Train Accuracy')\n",
                "plt.plot(history['test_acc'], label='Test (Validation) Accuracy')\n",
                "plt.title('Accuracy')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 모델 저장\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    X_test_device = X_test_t.to(device)\n",
                "    outputs = model(X_test_device)\n",
                "    _, predicted = torch.max(outputs, 1)\n",
                "    y_pred = predicted.cpu().numpy()\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=['Neutral', 'Anxious']))\n",
                "\n",
                "torch.save(model.state_dict(), 'emotion_model_gpu.pth')\n",
                "print(\"Model saved.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

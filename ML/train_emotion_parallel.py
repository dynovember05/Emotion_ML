import os
import glob
import zipfile
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from concurrent.futures import ProcessPoolExecutor
import mediapipe as mp
import cv2
from tqdm import tqdm

# ==========================================
# 1. ì„¤ì • (Configuration)
# ==========================================
# ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
BASE_PATH = r"C:\Users\ldy34\Desktop\Face"
TRAIN_DATA_PATH = os.path.join(BASE_PATH, "video", "Training")

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
BATCH_SIZE = 128    # ë©”ëª¨ë¦¬ê°€ ë„‰ë„‰í•˜ë¯€ë¡œ í¬ê²Œ ì„¤ì •
EPOCHS = 50
LEARNING_RATE = 0.001
NUM_WORKERS = os.cpu_count()  # CPU ì½”ì–´ ìˆ˜ë§Œí¼ ë³‘ë ¬ ì²˜ë¦¬

# ë¯¸ë””ì–´íŒŒì´í”„ ì„¤ì •
mp_face_mesh = mp.solutions.face_mesh

# ==========================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (Process)
# ==========================================
def process_image(args):
    """
    ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ZIP íŒŒì¼ ë‚´ì˜ ì´ë¯¸ì§€ë¥¼ ì½ì–´ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    zip_path, filename, label = args
    
    try:
        # ZIP íŒŒì¼ ì—´ê¸° (ì••ì¶• í•´ì œ ì—†ì´ ë©”ëª¨ë¦¬ ë¡œë“œ)
        with zipfile.ZipFile(zip_path, 'r') as zf:
            file_bytes = np.frombuffer(zf.read(filename), np.uint8)
            image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)

        if image is None:
            return None

        # MediaPipe FaceMesh ì ìš©
        with mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5
        ) as face_mesh:
            results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

            if not results.multi_face_landmarks:
                return None

            # ëœë“œë§ˆí¬ ì¶”ì¶œ (478ê°œ í¬ì¸íŠ¸)
            landmarks = results.multi_face_landmarks[0].landmark
            coords = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]) # (478, 3)

            # --- [ë°ì´í„° ì •ê·œí™”: Centering & Scaling] ---
            # 1. ì½” ë(ì¸ë±ìŠ¤ 1)ì„ ì›ì ìœ¼ë¡œ ì´ë™ (Centering)
            nose_tip = coords[1]
            coords -= nose_tip

            # 2. ì ˆëŒ€ê°’ì˜ ìµœëŒ€ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤ì¼€ì¼ë§ (Scaling)
            max_val = np.max(np.abs(coords))
            if max_val > 0:
                coords /= max_val
            
            # 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ (478 * 3 = 1434)
            return (coords.flatten().astype(np.float32), label)

    except Exception as e:
        # ì†ìƒëœ ì´ë¯¸ì§€ ë“±ì€ ë¬´ì‹œ
        return None

# ==========================================
# 3. ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ==========================================
class FaceEmotionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ==========================================
# 4. ëª¨ë¸ ì •ì˜ (MLP)
# ==========================================
class EmotionMLP(nn.Module):
    def __init__(self):
        super(EmotionMLP, self).__init__()
        self.input_size = 478 * 3 # 1434
        
        self.layer1 = nn.Sequential(
            nn.Linear(self.input_size, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.3)
        )
        self.layer2 = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.2)
        )
        self.layer3 = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU()
        )
        self.output = nn.Linear(128, 2) # [Neutral, Anxiety]

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return self.output(x)

# ==========================================
# 5. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ==========================================
if __name__ == '__main__':
    # ìœˆë„ìš° ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ ë°©ì§€
    import multiprocessing
    multiprocessing.freeze_support()

    print(f"[Start] í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘ (CPUs: {NUM_WORKERS})")
    print(f"ë°ì´í„° ê²½ë¡œ: {TRAIN_DATA_PATH}")

    # 1. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
    data_list = [] # (zip_path, filename, label)
    
    # globì„ ì‚¬ìš©í•˜ì—¬ ZIP íŒŒì¼ ì°¾ê¸° (íŒŒì¼ëª…ì— 'ì¤‘ë¦½' ë˜ëŠ” 'ë¶ˆì•ˆ'ì´ í¬í•¨ëœ íŒŒì¼)
    zip_files = glob.glob(os.path.join(TRAIN_DATA_PATH, "*.zip"))
    
    print(f"ë°œê²¬ëœ ZIP íŒŒì¼: {len(zip_files)}ê°œ")

    for zip_path in zip_files:
        filename_only = os.path.basename(zip_path)
        
        # ë¼ë²¨ë§: ì¤‘ë¦½=0, ë¶ˆì•ˆ=1
        if "ì¤‘ë¦½" in filename_only or "Neutral" in filename_only:
            label = 0
            label_name = "ì¤‘ë¦½(Neutral)"
        elif "ë¶ˆì•ˆ" in filename_only or "Anxiety" in filename_only:
            label = 1
            label_name = "ë¶ˆì•ˆ(Anxiety)"
        else:
            continue # í•´ë‹¹ ê°ì •ì´ ì•„ë‹ˆë©´ ìŠ¤í‚µ

        print(f"   -> Reading ZIP: {filename_only} ({label_name})")
        
        try:
            with zipfile.ZipFile(zip_path, 'r') as zf:
                # ì´ë¯¸ì§€ íŒŒì¼ë§Œ í•„í„°ë§ (jpg, png)
                image_files = [f for f in zf.namelist() if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                for img_file in image_files:
                    data_list.append((zip_path, img_file, label))
        except Exception as e:
            print(f"Error reading zip {zip_path}: {e}")

    print(f"ì´ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(data_list)}ì¥")
    
    # 2. ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëœë“œë§ˆí¬ ì¶”ì¶œ
    X_data = []
    y_data = []

    print("MediaPipe ëœë“œë§ˆí¬ ì¶”ì¶œ ì¤‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        results = list(tqdm(executor.map(process_image, data_list), total=len(data_list)))

    # None(ì–¼êµ´ ë¯¸ê²€ì¶œ) ì œê±° ë° ë°ì´í„° ë³‘í•©
    for res in results:
        if res is not None:
            X_data.append(res[0])
            y_data.append(res[1])

    X_data = np.array(X_data)
    y_data = np.array(y_data)

    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: ìœ íš¨ ë°ì´í„° {len(X_data)}ê°œ")
    
    # 3. Train / Test Split
    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)

    # ë°ì´í„°ì…‹ & ë°ì´í„°ë¡œë” ìƒì„±
    train_dataset = FaceEmotionDataset(X_train, y_train)
    test_dataset = FaceEmotionDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 4. ëª¨ë¸ ì´ˆê¸°í™” ë° GPU ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"í•™ìŠµ ì¥ì¹˜: {device}")
    
    model = EmotionMLP()
    
    # GPUê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš° DataParallel ì‚¬ìš©
    if torch.cuda.device_count() > 1:
        print(f"{torch.cuda.device_count()}ê°œì˜ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤! (DataParallel)")
        model = nn.DataParallel(model)
    
    model.to(device)

    # Loss & Optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 5. í•™ìŠµ ë£¨í”„
    best_acc = 0.0
    
    print("\nëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()

        # ê²€ì¦ (Validation)
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        epoch_acc = 100 * correct / total
        print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {running_loss/len(train_loader):.4f} | Accuracy: {epoch_acc:.2f}%")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if epoch_acc > best_acc:
            best_acc = epoch_acc
            # DataParallel ì‚¬ìš© ì‹œ moduleì— ì ‘ê·¼í•˜ì—¬ ì €ì¥
            save_model = model.module if isinstance(model, nn.DataParallel) else model
            torch.save(save_model.state_dict(), "best_emotion_model.pth")
            print(f"   --> Best Model Saved! ({best_acc:.2f}%)")

    print(f"\nğŸ† í•™ìŠµ ì¢…ë£Œ. ìµœì¢… ìµœê³  ì •í™•ë„: {best_acc:.2f}%")